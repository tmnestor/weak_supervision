{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save IHC case statements in an ordered dictionary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k5P8IqGdoUs6"
      },
      "source": [
        "Two case statements in IHC - 2021 v19 20221014 have been recoded programatically\n",
        "- the second appearance of Y has been recoded as Y1 because it has different logic, but both map to 96\n",
        "- the second appearance of BH has been recoded as BH1 because it has different logic, but both map to 9\n",
        "\n",
        "\n",
        "The IHC has two separate heirachies of 101 LFs into 99 labels\n",
        "- One heirachy for Cr_Expns_Descn_Txt\n",
        "- Another heirachy for Trvl_Expns_Descn_Txt, Self_Educn_Expns_Descn_Txt and WRE_Othr_Expns_Descn_Txt\n",
        "\n",
        "N.B. \"DL: Other_Work_Sites\" was commented out of the IHC on 20190213 so is not processed further in this script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory data already exists\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# set data directory location\n",
        "os.environ[\"DATA_DIR\"] = 'data'\n",
        "data_path=os.environ.get(\"DATA_DIR\")\n",
        "# create data directory if it does not exist\n",
        "try:\n",
        "    os.makedirs(data_path, exist_ok = False)\n",
        "    print(f\"Directory {data_path} created successfully\")\n",
        "except OSError as error:\n",
        "    print(f\"Directory {data_path} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K_OsWzuGoUs8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "infile = f\"{data_path}/IHC - 2021 v19 20221014.txt\"\n",
        "outfile = f'{data_path}/scratch1.txt'\n",
        "\n",
        "case_start = re.compile(r\"^WHEN\")\n",
        "case_stop = re.compile(r\"THEN\\s+'[A-Z1]{1,3}:\\s+[\\w\\s/$&-]+'\")\n",
        "\n",
        "extract_on = False\n",
        "case_statement_lines = []\n",
        "\n",
        "# strip non-case statements\n",
        "with open(infile, 'r') as infile:\n",
        "    for line in infile:\n",
        "        if case_start.match(line):\n",
        "            extract_on = True\n",
        "        if extract_on:\n",
        "            case_statement_lines.append(line.rstrip('\\n'))\n",
        "        if case_stop.search(line):\n",
        "            extract_on = False\n",
        "\n",
        "# save case statement line to file\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    for line in case_statement_lines:\n",
        "        # normalise non-standard WHEN clauses\n",
        "        line = re.sub(r\"WHEN\\s*\\n+\", r\"WHEN \", line)\n",
        "        out_fh.write(f\"{line}\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strip Comments (inline /\\*..\\*/ and -- as well as multiline /\\*..\\*/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Q2pTdXJvoUs8"
      },
      "outputs": [],
      "source": [
        "infile  = f'{data_path}/scratch1.txt'\n",
        "outfile  = f'{data_path}/scratch2.txt'\n",
        "comment_match1 = re.compile(r\"/\\*.*?\\*/\")\n",
        "comment_match2 = re.compile(r\"--+.*$\")\n",
        "\n",
        "# strip single line comments\n",
        "with open(outfile, 'w') as out_fh:\n",
        "  with open(infile, 'r') as in_fh:\n",
        "    for line in in_fh:\n",
        "      line = comment_match1.sub(r'', line)\n",
        "      line = comment_match2.sub(r'', line)\n",
        "      out_fh.write(f\"{line}\")\n",
        "\n",
        "infile = f'{data_path}/scratch2.txt'\n",
        "outfile = f'{data_path}/scratch3.txt'\n",
        "multiline_comment_match = re.compile(r\"^\\/\\*[\\s\\S]+?\\*\\/\", re.MULTILINE)\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "# strip multi line comments\n",
        "case_statements = multiline_comment_match.sub(r'', case_statements)\n",
        "case_statements = multiline_comment_match.sub(r'', case_statements)\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    out_fh.write(case_statements)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standardise format of Teradata RegExp_Similar statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "infile = f'{data_path}/scratch3.txt'\n",
        "outfile = f'{data_path}/scratch4.txt'\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "\n",
        "case_statements = re.sub(r\"WHEN\\s+\\nRegExp_Similar\",\n",
        "                         r\"WHEN RegExp_Similar\", case_statements)\n",
        "case_statements = re.sub(r\"WHEN\\s+\\n\\(\\nRegExp_Similar\",\n",
        "                         r\"WHEN (RegExp_Similar\", case_statements)\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    out_fh.write(case_statements)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Raw Case Statements (minus comments, but with uniform formatting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "euJznMeGoUs9"
      },
      "outputs": [],
      "source": [
        "infile = f'{data_path}/scratch4.txt'\n",
        "outfile = f'{data_path}/raw_case_statements.txt'\n",
        "\n",
        "# strip blanklines\n",
        "with open(outfile, 'w') as out_fh:\n",
        "  with open(infile, 'r') as in_fh:\n",
        "    for line in in_fh:\n",
        "      if not line.isspace():\n",
        "        out_fh.write(f\"{line}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Key Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wl93JFIWoUs9",
        "outputId": "8f4e0a18-9b68-4831-95e5-09e3a72b96b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DV:4\n",
            "BG:4\n",
            "A:4\n",
            "B:4\n",
            "E:4\n",
            "CI:4\n",
            "BS:4\n",
            "DK:4\n",
            "C:4\n",
            "DI:4\n",
            "F:4\n",
            "G:4\n",
            "H:4\n",
            "J:4\n",
            "I:4\n",
            "V:4\n",
            "D:4\n",
            "BH:8\n",
            "K:4\n",
            "L:4\n",
            "M:4\n",
            "N:4\n",
            "O:4\n",
            "P:4\n",
            "Q:4\n",
            "R:4\n",
            "U:4\n",
            "S:4\n",
            "T:4\n",
            "CK:4\n",
            "W:4\n",
            "X:4\n",
            "Y:8\n",
            "Z:4\n",
            "BA:4\n",
            "BB:4\n",
            "BC:4\n",
            "BF:4\n",
            "BI:4\n",
            "BJ:4\n",
            "BK:4\n",
            "BL:4\n",
            "BM:4\n",
            "BN:4\n",
            "BO:4\n",
            "BQ:4\n",
            "BR:4\n",
            "AM:4\n",
            "BT:4\n",
            "BU:4\n",
            "BV:4\n",
            "BW:4\n",
            "BX:4\n",
            "BY:4\n",
            "BZ:4\n",
            "CA:4\n",
            "CB:4\n",
            "CC:4\n",
            "CD:4\n",
            "CE:4\n",
            "CF:4\n",
            "CG:4\n",
            "CH:4\n",
            "CJ:4\n",
            "CM:4\n",
            "CN:4\n",
            "CO:4\n",
            "CP:4\n",
            "CQ:4\n",
            "CR:4\n",
            "CS:4\n",
            "CT:4\n",
            "CU:4\n",
            "CV:4\n",
            "CW:4\n",
            "CX:4\n",
            "CY:4\n",
            "CZ:4\n",
            "DA:4\n",
            "DB:4\n",
            "DC:4\n",
            "DD:4\n",
            "DE:4\n",
            "DF:4\n",
            "DG:4\n",
            "DH:4\n",
            "DJ:4\n",
            "DT:4\n",
            "CL:4\n",
            "BD:4\n",
            "BP:4\n",
            "DM:4\n",
            "ZZZ:4\n",
            "DO:4\n",
            "DN:4\n",
            "DP:4\n",
            "DQ:4\n",
            "DR:4\n",
            "DS:4\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "infile = f'{data_path}/raw_case_statements.txt'\n",
        "outfile = f'{data_path}/wre_code_counts.txt'\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "\n",
        "# code_pattern = r\"THEN\\s+'([A-Z1]{1,3}):\\s+[\\w\\s/$&-]+'\"\n",
        "code_pattern = r\"(?:THEN\\s+')([A-Z1]{1,3}):(?:\\s+[\\w\\s/$&-]+')\"\n",
        "\n",
        "ptrn = re.compile(code_pattern, re.MULTILINE)\n",
        "codes = ptrn.findall(case_statements)\n",
        "\n",
        "# print(codes)\n",
        "\n",
        "code_map = Counter(codes)\n",
        "\n",
        "# get the key counts\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    for key, value in code_map.items():\n",
        "        print(f\"{key}:{value}\")\n",
        "        out_fh.write(f\"{key}:{value}\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse Case Statements into clauses and store in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "infile = f'{data_path}/raw_case_statements.txt'\n",
        "# extract the case statements\n",
        "case_statement_pattern = r\"(?P<case_statement>^WHEN(?P<logic>[\\s\\S\\n]+?)THEN\\s+'(?P<wre_code>[A-Z1]{1,3}):\\s+(?P<code_desc>[\\w\\s/$&-]+)')\"\n",
        "case_ptrn = re.compile(case_statement_pattern, re.MULTILINE)\n",
        "\n",
        "# extract the case statement wre code\n",
        "code_pattern = r\"^THEN\\s+'(?P<wre_code>[A-Z1]{1,3}):\\s+[\\w\\s/$&-]+'\"\n",
        "code_ptrn = re.compile(code_pattern, re.MULTILINE)\n",
        "\n",
        "# extract the wre code's description\n",
        "code_descr_rgx = r\"THEN\\s+'[A-Z1]{1,3}:\\s+(?P<code_desc>[\\w\\s/$&-]+)'\"\n",
        "code_descr_ptrn = re.compile(code_descr_rgx)\n",
        "\n",
        "# extract the Teradata RegExpSimilar clause\n",
        "regexp_rgx = r\"\\('\\s*\\|\\|\\s*'(?P<regexp>[\\w|\\n]+)'\\s*\\|\\|\\s*'\\)\"\n",
        "regexp_ptrn = re.compile(regexp_rgx, re.MULTILINE)\n",
        "\n",
        "# extract the case statement's LIKE ANY clause\n",
        "like_any_rgx = r\"LIKE ANY\\s*(?P<like_any>\\([\\w\\W\\n]+?\\))\\s*\\n(AND|OR|THEN)\"\n",
        "like_any_ptrn = re.compile(like_any_rgx, re.MULTILINE)\n",
        "\n",
        "# extract the case statement's NOT LIKE ALL clause\n",
        "not_like_all_rgx = r\"NOT LIKE ALL\\s*(?P<not_like_all>\\([\\w\\W\\n]+?\\))[\\n]*?\\s*?(THEN|OR)\"\n",
        "not_like_all_ptrn = re.compile(not_like_all_rgx, re.MULTILINE)\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements_string = in_fh.read()\n",
        "\n",
        "NUM_CASES = 101  # includes Y, Y, BH and BH\n",
        "#########################################################################\n",
        "# define IHC_dict dictionary to catalogue IHC case statements\n",
        "#########################################################################\n",
        "case_statement_list = [m.groupdict()\n",
        "                       for m in case_ptrn.finditer(case_statements_string)]\n",
        "Cr_Expns_order = case_statement_list[:NUM_CASES]\n",
        "not_Cr_Expns_order = case_statement_list[NUM_CASES:2*NUM_CASES]\n",
        "# codes3 = case_statement_list[2*NUM_CASES:3*NUM_CASES]\n",
        "# codes4 = case_statement_list[3*NUM_CASES:4*NUM_CASES]\n",
        "\n",
        "# for i in range(101):\n",
        "#     print(f\"{Cr_Expns_order[i]['wre_code']}\\t{codes2[i]['wre_code']}\\t{codes3[i]['wre_code']}\\t{codes4[i]['wre_code']}\")\n",
        "\n",
        "codes_seen = []\n",
        "for i in range(NUM_CASES):\n",
        "    current_code = Cr_Expns_order[i]['wre_code']\n",
        "    codes_seen.append(current_code)\n",
        "    if current_code in codes_seen[:i]:\n",
        "        current_code = current_code+'1'\n",
        "        Cr_Expns_order[i]['wre_code'] = current_code\n",
        "    # print(f\"{Cr_Expns_order[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "IHC_dict = {statement['wre_code']: statement['case_statement']\n",
        "            for statement in Cr_Expns_order}\n",
        "\n",
        "\n",
        "\n",
        "preprocess1 = re.compile(r\"(\\w+)(%+)(\\w+)\")\n",
        "preprocess2 = re.compile(r\"['\\n()%]\")\n",
        "def strip_char(text):\n",
        "    text = preprocess1.sub(r'\\1 \\3', text)\n",
        "    text = preprocess2.sub('', text)\n",
        "    text = text.strip()\n",
        "    # text = ' '.join(preprocess.findall(text.lower())).strip()\n",
        "    # text = re.sub(r\"['\\n\\(\\)]\", ' ', text)\n",
        "    # text = re.sub(r\"(\\w+)(%+)(\\w+)\", r'\\1 \\3', text)\n",
        "    # text = re.sub(r\"(\\w+)(%+)(\\w+)\", r'\\1 \\3', text)\n",
        "    return text\n",
        "\n",
        "def assemble_clause(search_term):\n",
        "    search_term = [strip_char(text) for text in re.split(',', search_term)]\n",
        "    search_term = \"|\".join(search_term)\n",
        "    # print(f\"{search_term=}\")\n",
        "    return search_term\n",
        "\n",
        "# split each case statement into separate clauses\n",
        "for code, case_statement in IHC_dict.items():\n",
        "    # print(f\"{code=}\")\n",
        "    code_descr = code_descr_ptrn.search(case_statement)\n",
        "    code_descr = code_descr.group('code_desc') if bool(code_descr) else None\n",
        "\n",
        "    like_any = like_any_ptrn.search(case_statement)\n",
        "    like_any = assemble_clause(like_any.group(\n",
        "        'like_any')) if bool(like_any) else None\n",
        "    # print(f\"{like_any=}\")\n",
        "\n",
        "    regexp = regexp_ptrn.search(case_statement)\n",
        "    regexp = regexp.group('regexp') if bool(regexp) else None\n",
        "    # print(f\"{regexp=}\")\n",
        "\n",
        "    not_like_all = not_like_all_ptrn.search(case_statement)\n",
        "    not_like_all = assemble_clause(not_like_all.group(\n",
        "        'not_like_all')) if bool(not_like_all) else None\n",
        "    # print(f\"{not_like_all=}\")\n",
        "\n",
        "    #########################################################################\n",
        "    # redefine IHC_dict dictionary fields\n",
        "    #########################################################################\n",
        "    IHC_dict[code] = {\"code_descr\": code_descr,\n",
        "                      \"like_any\": like_any,\n",
        "                      \"regexp\": regexp,\n",
        "                      \"not_like_all\": not_like_all}\n",
        "\n",
        "\n",
        "# helper function to return case statemnt clauses\n",
        "def get_case_statement(wre_code, component=None):\n",
        "    # if component is not None:\n",
        "    if bool(component):\n",
        "        return IHC_dict[wre_code][component]\n",
        "    else:\n",
        "        return IHC_dict[wre_code]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DaXGx53IoUs_",
        "outputId": "940491cd-92f3-4eb3-ecff-a0f4bbac0891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'code_descr': 'Parking/Tolls', 'like_any': 'parking|toll|car park|CARPARK|ETAG|motorway|citylink|CITY LINK|CITI LINK|eastlink|LINKT|park|EAST LINK|PARKIG|PARKIGN|PARKIING|PARKIMG|PARKIN|PARKINH|PARKINING|PARK|CAR PARKS|E TAG|WILSON|WILSONS|PAKING', 'regexp': None, 'not_like_all': 'royal|plaza|regis|tower|national park|SPARK|postage|percentage|security tag|advantage|name tag|instagram|parker pen|LEAD TAGGING|LUNA PARK|TOLLS OF TRADE|PAKING BOXES|ALBERT PARK|ERSKIN PARK|EXL CAR PARKS|MACQUARIE PARK|DUDLEY PARK|NATURE PARK|PARKES|PARK MANAGEMENT|PARKVILLE|ROSNY PARK|STUART PARK|SHIFT PARK TO|ARIAH PARK|PRESIDENTS PARK|ERSKINE PARK'}\n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('Y'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BPLyOV8NoUs_",
        "outputId": "be971fac-0979-4a28-8737-52f87bd44174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'code_descr': 'Parking/Tolls', 'like_any': 'ALBERT PARK PARKING|ERSKIN PARK PARKING|EXL CAR PARKS PARKING|MACQUARIE PARK PARKING|DUDLEY PARK PARKING|NATURE PARK PARKING|PARKES PARKING|PARK MANAGEMENT PARKING|PARKVILLE PARKING|ROSNY PARK PARKING|STUART PARK PARKING|SHIFT PARK PARKING|ARIAH PARK PARKING|PRESIDENTS PARK PARKING|ERSKINE PARK PARKING PARKING ALBERT PARK|PARKING ERSKIN PARK|PARKING EXL CAR PARKS|PARKING MACQUARIE PARK|PARKING DUDLEY PARK|PARKING NATURE PARK|PARKING PARKES|PARKING PARK MANAGEMENT|PARKING PARKVILLE|PARKING ROSNY PARK|PARKING STUART PARK|PARKING SHIFT PARK TO|PARKING ARIAH PARK|PARKING PRESIDENTS PARK|PARKING ERSKINE PARK', 'regexp': None, 'not_like_all': None}\n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('Y1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PHIjFruioUs_",
        "outputId": "c51cfb97-cf41-4be2-d02a-ff7bd00a3574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('H', 'regexp'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dump IHC_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-HfUsStMoUtA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "outfile = f'{data_path}/IHC_dict.txt'\n",
        "\n",
        "# write text description of case statement dictionary to file\n",
        "with open(outfile, 'w') as case_statement_fh:\n",
        "    case_statement_fh.write(json.dumps(IHC_dict))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "622930c0a846cc3025e2933c7b0895291b5050626809fa6d7200ae14613946b7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
