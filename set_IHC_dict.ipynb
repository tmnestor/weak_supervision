{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save IHC case statement in a Dictionary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k5P8IqGdoUs6"
      },
      "source": [
        "Two case statements in IHC - 2021 v19 20221014 have been hand edited\n",
        "- the second appearance of Y has been recoded as Y1 because it has different logic\n",
        "- the second appearance of BH has been recoded as BH1 because it has different logic\n",
        "\n",
        "\"DL: Other_Work_Sites\" was commented out of the IHC on 20190213"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory data already exists\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# set data directory location\n",
        "os.environ[\"DATA_DIR\"] = 'data'\n",
        "data_path=os.environ.get(\"DATA_DIR\")\n",
        "# create data directory if it does not exist\n",
        "try:\n",
        "    os.makedirs(data_path, exist_ok = False)\n",
        "    print(f\"Directory {data_path} created successfully\")\n",
        "except OSError as error:\n",
        "    print(f\"Directory {data_path} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K_OsWzuGoUs8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "infile = f\"{data_path}/IHC - 2021 v19 20221014.txt\"\n",
        "outfile = f'{data_path}/scratch1.txt'\n",
        "\n",
        "case_start = re.compile(r\"^WHEN\")\n",
        "case_stop = re.compile(r\"THEN\\s+'[A-Z1]{1,3}:\\s+[\\w\\s/$&-]+'\")\n",
        "\n",
        "extract_on = False\n",
        "case_statement_lines = []\n",
        "\n",
        "# strip non-case statements\n",
        "with open(infile, 'r') as infile:\n",
        "    for line in infile:\n",
        "        if case_start.match(line):\n",
        "            extract_on = True\n",
        "        if extract_on:\n",
        "            case_statement_lines.append(line.rstrip('\\n'))\n",
        "        if case_stop.search(line):\n",
        "            extract_on = False\n",
        "\n",
        "# save case statement line to file\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    for line in case_statement_lines:\n",
        "        # normalise non-standard WHEN clauses\n",
        "        line = re.sub(r\"WHEN\\s*\\n+\", r\"WHEN \", line)\n",
        "        out_fh.write(f\"{line}\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strip Comments (inline /\\*..\\*/ and -- as well as multiline /\\*..\\*/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q2pTdXJvoUs8"
      },
      "outputs": [],
      "source": [
        "infile  = f'{data_path}/scratch1.txt'\n",
        "outfile  = f'{data_path}/scratch2.txt'\n",
        "comment_match1 = re.compile(r\"/\\*.*?\\*/\")\n",
        "comment_match2 = re.compile(r\"--+.*$\")\n",
        "\n",
        "# strip single line comments\n",
        "with open(outfile, 'w') as out_fh:\n",
        "  with open(infile, 'r') as in_fh:\n",
        "    for line in in_fh:\n",
        "      line = comment_match1.sub(r'', line)\n",
        "      line = comment_match2.sub(r'', line)\n",
        "      out_fh.write(f\"{line}\")\n",
        "\n",
        "infile = f'{data_path}/scratch2.txt'\n",
        "outfile = f'{data_path}/scratch3.txt'\n",
        "multiline_comment_match = re.compile(r\"^\\/\\*[\\s\\S]+?\\*\\/\", re.MULTILINE)\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "# strip multi line comments\n",
        "case_statements = multiline_comment_match.sub(r'', case_statements)\n",
        "case_statements = multiline_comment_match.sub(r'', case_statements)\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    out_fh.write(case_statements)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standardise format of Teradata RegExp_Similar statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "infile = f'{data_path}/scratch3.txt'\n",
        "outfile = f'{data_path}/scratch4.txt'\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "\n",
        "case_statements = re.sub(r\"WHEN\\s+\\nRegExp_Similar\",\n",
        "                         r\"WHEN RegExp_Similar\", case_statements)\n",
        "case_statements = re.sub(r\"WHEN\\s+\\n\\(\\nRegExp_Similar\",\n",
        "                         r\"WHEN (RegExp_Similar\", case_statements)\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    out_fh.write(case_statements)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Raw Case Statements (minus comments, but with uniform formatting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "euJznMeGoUs9"
      },
      "outputs": [],
      "source": [
        "infile = f'{data_path}/scratch4.txt'\n",
        "outfile = f'{data_path}/raw_case_statements.txt'\n",
        "\n",
        "# strip blanklines\n",
        "with open(outfile, 'w') as out_fh:\n",
        "  with open(infile, 'r') as in_fh:\n",
        "    for line in in_fh:\n",
        "      if not line.isspace():\n",
        "        out_fh.write(f\"{line}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Key Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wl93JFIWoUs9",
        "outputId": "8f4e0a18-9b68-4831-95e5-09e3a72b96b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DV:4\n",
            "BG:4\n",
            "A:4\n",
            "B:4\n",
            "E:4\n",
            "CI:4\n",
            "BS:4\n",
            "DK:4\n",
            "C:4\n",
            "DI:4\n",
            "F:4\n",
            "G:4\n",
            "H:4\n",
            "J:4\n",
            "I:4\n",
            "V:4\n",
            "D:4\n",
            "BH:4\n",
            "K:4\n",
            "L:4\n",
            "M:4\n",
            "N:4\n",
            "O:4\n",
            "P:4\n",
            "Q:4\n",
            "R:4\n",
            "U:4\n",
            "S:4\n",
            "T:4\n",
            "CK:4\n",
            "W:4\n",
            "X:4\n",
            "Y:4\n",
            "Y1:4\n",
            "Z:4\n",
            "BA:4\n",
            "BB:4\n",
            "BC:4\n",
            "BF:4\n",
            "BI:4\n",
            "BJ:4\n",
            "BK:4\n",
            "BL:4\n",
            "BM:4\n",
            "BN:4\n",
            "BO:4\n",
            "BQ:4\n",
            "BR:4\n",
            "AM:4\n",
            "BT:4\n",
            "BU:4\n",
            "BV:4\n",
            "BW:4\n",
            "BX:4\n",
            "BY:4\n",
            "BZ:4\n",
            "CA:4\n",
            "CB:4\n",
            "CC:4\n",
            "CD:4\n",
            "CE:4\n",
            "CF:4\n",
            "CG:4\n",
            "CH:4\n",
            "CJ:4\n",
            "CM:4\n",
            "CN:4\n",
            "CO:4\n",
            "CP:4\n",
            "CQ:4\n",
            "CR:4\n",
            "CS:4\n",
            "CT:4\n",
            "CU:4\n",
            "CV:4\n",
            "CW:4\n",
            "CX:4\n",
            "CY:4\n",
            "CZ:4\n",
            "DA:4\n",
            "DB:4\n",
            "DC:4\n",
            "DD:4\n",
            "DE:4\n",
            "DF:4\n",
            "DG:4\n",
            "DH:4\n",
            "DJ:4\n",
            "DT:4\n",
            "CL:4\n",
            "BD:4\n",
            "BP:4\n",
            "DM:4\n",
            "BH1:4\n",
            "ZZZ:4\n",
            "DO:4\n",
            "DN:4\n",
            "DP:4\n",
            "DQ:4\n",
            "DR:4\n",
            "DS:4\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "infile = f'{data_path}/raw_case_statements.txt'\n",
        "outfile = f'{data_path}/wre_code_counts.txt'\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "\n",
        "# code_pattern = r\"THEN\\s+'([A-Z1]{1,3}):\\s+[\\w\\s/$&-]+'\"\n",
        "code_pattern = r\"(?:THEN\\s+')([A-Z1]{1,3}):(?:\\s+[\\w\\s/$&-]+')\"\n",
        "\n",
        "ptrn = re.compile(code_pattern, re.MULTILINE)\n",
        "codes = ptrn.findall(case_statements)\n",
        "\n",
        "# print(codes)\n",
        "\n",
        "code_map = Counter(codes)\n",
        "\n",
        "# get the key counts\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    for key, value in code_map.items():\n",
        "        print(f\"{key}:{value}\")\n",
        "        out_fh.write(f\"{key}:{value}\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse Case Statements into clauses and store in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "infile = f'{data_path}/raw_case_statements.txt'\n",
        "# extract the case statements\n",
        "case_statement_pattern = r\"(?P<case_statement>^WHEN(?P<logic>[\\s\\S\\n]+?)THEN\\s+'(?P<wre_code>[A-Z1]{1,3}):\\s+(?P<code_desc>[\\w\\s/$&-]+)')\"\n",
        "case_ptrn = re.compile(case_statement_pattern, re.MULTILINE)\n",
        "\n",
        "# extract the case statement wre code\n",
        "code_pattern = r\"^THEN\\s+'(?P<wre_code>[A-Z1]{1,3}):\\s+[\\w\\s/$&-]+'\"\n",
        "code_ptrn = re.compile(code_pattern, re.MULTILINE)\n",
        "\n",
        "# extract the wre code's description\n",
        "code_descr_rgx = r\"THEN\\s+'[A-Z1]{1,3}:\\s+(?P<code_desc>[\\w\\s/$&-]+)'\"\n",
        "code_descr_ptrn = re.compile(code_descr_rgx)\n",
        "\n",
        "# extract the Teradata RegExpSimilar clause\n",
        "regexp_rgx = r\"\\('\\s*\\|\\|\\s*'(?P<regexp>[\\w|\\n]+)'\\s*\\|\\|\\s*'\\)\"\n",
        "regexp_ptrn = re.compile(regexp_rgx, re.MULTILINE)\n",
        "\n",
        "# extract the case statement's LIKE ANY clause\n",
        "like_any_rgx = r\"LIKE ANY\\s*(?P<like_any>\\([\\w\\W\\n]+?\\))\\s*\\n(AND|OR|THEN)\"\n",
        "like_any_ptrn = re.compile(like_any_rgx, re.MULTILINE)\n",
        "\n",
        "# extract the case statement's NOT LIKE ALL clause\n",
        "not_like_all_rgx = r\"NOT LIKE ALL\\s*(?P<not_like_all>\\([\\w\\W\\n]+?\\))[\\n]*?\\s*?(THEN|OR)\"\n",
        "not_like_all_ptrn = re.compile(not_like_all_rgx, re.MULTILINE)\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements_string = in_fh.read()\n",
        "\n",
        "NUM_CASES = 101  # includes Y, Y1, BH and BH1\n",
        "#########################################################################\n",
        "# define IHC_dict dictionary to catalogue IHC case statements\n",
        "#########################################################################\n",
        "case_statement_list = [m.groupdict()\n",
        "                       for m in case_ptrn.finditer(case_statements_string)]\n",
        "IHC_dict = {statement['wre_code']: statement['case_statement']\n",
        "            for statement in case_statement_list[:NUM_CASES]}\n",
        "\n",
        "preprocess1 = re.compile(r\"(\\w+)(%+)(\\w+)\")\n",
        "preprocess2 = re.compile(r\"['\\n()%]\")\n",
        "def strip_char(text):\n",
        "    text = preprocess1.sub(r'\\1 \\3', text)\n",
        "    text = preprocess2.sub('', text)\n",
        "    text = text.strip()\n",
        "    # text = ' '.join(preprocess.findall(text.lower())).strip()\n",
        "    # text = re.sub(r\"['\\n\\(\\)]\", ' ', text)\n",
        "    # text = re.sub(r\"(\\w+)(%+)(\\w+)\", r'\\1 \\3', text)\n",
        "    # text = re.sub(r\"(\\w+)(%+)(\\w+)\", r'\\1 \\3', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def assemble_clause(search_term):\n",
        "    search_term = [strip_char(text) for text in re.split(',', search_term)]\n",
        "    search_term = \"|\".join(search_term)\n",
        "    # print(f\"{search_term=}\")\n",
        "    return search_term\n",
        "\n",
        "\n",
        "# split each case statement into separate clauses\n",
        "for code, case_statement in IHC_dict.items():\n",
        "    # print(f\"{code=}\")\n",
        "    code_descr = code_descr_ptrn.search(case_statement)\n",
        "    code_descr = code_descr.group('code_desc') if bool(code_descr) else None\n",
        "\n",
        "    like_any = like_any_ptrn.search(case_statement)\n",
        "    like_any = assemble_clause(like_any.group(\n",
        "        'like_any')) if bool(like_any) else None\n",
        "    # print(f\"{like_any=}\")\n",
        "\n",
        "    regexp = regexp_ptrn.search(case_statement)\n",
        "    regexp = regexp.group('regexp') if bool(regexp) else None\n",
        "    # print(f\"{regexp=}\")\n",
        "\n",
        "    not_like_all = not_like_all_ptrn.search(case_statement)\n",
        "    not_like_all = assemble_clause(not_like_all.group(\n",
        "        'not_like_all')) if bool(not_like_all) else None\n",
        "    # print(f\"{not_like_all=}\")\n",
        "\n",
        "    #########################################################################\n",
        "    # redefine IHC_dict dictionary fields\n",
        "    #########################################################################\n",
        "    IHC_dict[code] = {\"code_descr\": code_descr,\n",
        "                      \"like_any\": like_any,\n",
        "                      \"regexp\": regexp,\n",
        "                      \"not_like_all\": not_like_all}\n",
        "\n",
        "\n",
        "# helper function to return case statemnt clauses\n",
        "def get_case_statement(wre_code, component=None):\n",
        "    # if component is not None:\n",
        "    if bool(component):\n",
        "        return IHC_dict[wre_code][component]\n",
        "    else:\n",
        "        return IHC_dict[wre_code]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'code_descr': 'Parking/Tolls', 'like_any': ' ALBERT PARK PARKING| ERSKIN PARK PARKING| EXL CAR PARKS PARKING| MACQUARIE PARK PARKING| DUDLEY PARK PARKING| NATURE PARK PARKING| PARKES PARKING| PARK MANAGEMENT PARKING| PARKVILLE PARKING| ROSNY PARK PARKING| STUART PARK PARKING| SHIFT PARK PARKING| ARIAH PARK PARKING| PRESIDENTS PARK PARKING| ERSKINE PARK PARKING PARKING ALBERT PARK| PARKING ERSKIN PARK| PARKING EXL CAR PARKS| PARKING MACQUARIE PARK| PARKING DUDLEY PARK| PARKING NATURE PARK| PARKING PARKES| PARKING PARK MANAGEMENT| PARKING PARKVILLE| PARKING ROSNY PARK| PARKING STUART PARK| PARKING SHIFT PARK TO| PARKING ARIAH PARK| PARKING PRESIDENTS PARK| PARKING ERSKINE PARK', 'regexp': None, 'not_like_all': None}\n"
          ]
        }
      ],
      "source": [
        "print(IHC_dict['Y1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DaXGx53IoUs_",
        "outputId": "940491cd-92f3-4eb3-ecff-a0f4bbac0891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Home Office\n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('H', 'code_descr'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BPLyOV8NoUs_",
        "outputId": "be971fac-0979-4a28-8737-52f87bd44174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "home| home office| homeoffice|Home Study| home as office| home based office| work from home|working from home| WORKING AT HOME|Work at home| WFH| WORKFROMHOME| STUDY FURNITURE| STUDY ROOM|HLP| HLP| HO| HOE|HOE| H.O.E | H/O| H/O| H O 1000*0.45| H.O.E|H|HO |H/L/P|L & P|LP| H.O.E.| H O E| H O| HO: 10 HRS/WK| HO5X48X0.52| H.O EX| HM OFF| H.O| H.O 2HRS X 48WKS X $0.52| H.O 8HRS X 48WKS X $0.52| H.O.X.| HOM OFF| H.O.X| HO: 15 H/W| HO: 10 H/W| HO: 20 HRS/WK| HOU 5 X 48 X .52| HO: 5 HRS/WK| HOU 4 X 48 X .52| HOU 3 X 48 X .52| H O 100*0.45| H OFFIS| HO: 15 HRS/WK| H.O 10HRS X 48WKS X $0.52| HOU 10 X 48 X .52| HO: 6HRS P/WK*48WKS*.52| H.O. 2HRS X 48WKS X 0.52| H.O 2*5*48W*.52| H.O.| HO: 5 H/W| H.O 3HRS X 48WKS X $0.52| H.O. 15 HRS X 48 WKS @ 52C| H O EXP| H.O 10H*48W*.52| H.O 10HRS X 40WKS X $0.52| HOU 8 X 48 X .52| H.O 5HRS X 48WKS X $0.52| HO: 5HRS*48*45C| H O 1000*0.55| HMOFF| HO10X48X0.52| H.O 1*5*48W*.52| H.O 5HRS X 40WKS X $0.52| HO: 4HRS P/WK*48WKS*.52| H.O 4HRS X 48WKS X $0.52| HO: 3HRS P/WK*48WKS*.52| HO: 2 H/W| H.O. 20 HRS X 48 WKS @ 52C| H O 1000*045| H OFF| H O| H.O | HM OFF| HO-| HO| HO:| HO HR | HO PW | HO WKS | HO WEEK | HOFF | HOME OFFICE|WFH ALLOWANCE |HME OFF|H/OFFICE |HIME OFFICE |HOFFICE |H-OFFICE| HO|WFH EXPENSES|X52CENTS |APP: WFH |FOOTREST |WFH RUNNING EXPENSES |WFH  |WFH- |WFHO |WFHEXP |WFH| JOB SEEKING - WFH - COVID| COVID19 EXTRA HRS | COVID-19| COVID19| COVID| FIXED RATE| FIXED RATE METHOD| FIXED RATE WFH| HOME OFFICE - NOT COVID RELATE| HOME OFFICE CALC NON COVID| SHORTCUT| USE OF HOME OFFIC PRE COVID| WFH NON-COVID 40HRS| WFH PRIOR TO COVID 19| WFH PRIOR TO COVID 19| WORKING FORM HOME BEFORE COVID| HL&P| COVID | SHORTCUT | SHORT CUT | 80 C | FIXED RATE | 0.80 | MAR JUN | 52C | FIXED HOURLY RATE | X 80C | APR JUN | L&P | *80 C | WFH | 15 WKS | 16 WKS | 17 WKS | 18 WKS \n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('H', 'like_any'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PHIjFruioUs_",
        "outputId": "c51cfb97-cf41-4be2-d02a-ff7bd00a3574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('H', 'regexp'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-HfUsStMoUtA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "outfile = f'{data_path}/IHC_dict.txt'\n",
        "\n",
        "# write text description of case statement dictionary to file\n",
        "with open(outfile, 'w') as case_statement_fh:\n",
        "    case_statement_fh.write(json.dumps(IHC_dict))\n",
        "\n",
        "# read text description of case statement dictionary from file\n",
        "with open(outfile, 'r') as f:\n",
        "    dictionary_as_text = f.read()\n",
        "\n",
        "# reconstruct the case statement dictionary from its text description\n",
        "IHC_dict = json.loads(dictionary_as_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "622930c0a846cc3025e2933c7b0895291b5050626809fa6d7200ae14613946b7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
