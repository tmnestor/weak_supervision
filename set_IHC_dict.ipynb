{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save IHC case statement in a Dictionary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k5P8IqGdoUs6"
      },
      "source": [
        "Two case statements in IHC - 2021 v19 20221014 have been hand edited\n",
        "- the second appearance of Y has been recoded as Y1 because it has different logic\n",
        "- the second appearance of BH has been recoded as BH1 because it has different logic\n",
        "\n",
        "\"DL: Other_Work_Sites\" was commented out of the IHC on 20190213"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory data already exists\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# set data directory location\n",
        "os.environ[\"DATA_DIR\"] = 'data'\n",
        "data_path=os.environ.get(\"DATA_DIR\")\n",
        "# create data directory if it does not exist\n",
        "try:\n",
        "    os.makedirs(data_path, exist_ok = False)\n",
        "    print(f\"Directory {data_path} created successfully\")\n",
        "except OSError as error:\n",
        "    print(f\"Directory {data_path} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K_OsWzuGoUs8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "infile = f\"{data_path}/IHC - 2021 v19 20221014.txt\"\n",
        "outfile = f'{data_path}/scratch1.txt'\n",
        "\n",
        "case_start = re.compile(r\"^WHEN\")\n",
        "case_stop = re.compile(r\"THEN\\s+'[A-Z1]{1,3}:\\s+[\\w\\s/$&-]+'\")\n",
        "\n",
        "extract_on = False\n",
        "case_statement_lines = []\n",
        "\n",
        "# strip non-case statements\n",
        "with open(infile, 'r') as infile:\n",
        "    for line in infile:\n",
        "        if case_start.match(line):\n",
        "            extract_on = True\n",
        "        if extract_on:\n",
        "            case_statement_lines.append(line.rstrip('\\n'))\n",
        "        if case_stop.search(line):\n",
        "            extract_on = False\n",
        "\n",
        "# save case statement line to file\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    for line in case_statement_lines:\n",
        "        # normalise non-standard WHEN clauses\n",
        "        line = re.sub(r\"WHEN\\s*\\n+\", r\"WHEN \", line)\n",
        "        out_fh.write(f\"{line}\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strip Comments (inline /\\*..\\*/ and -- as well as multiline /\\*..\\*/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q2pTdXJvoUs8"
      },
      "outputs": [],
      "source": [
        "infile  = f'{data_path}/scratch1.txt'\n",
        "outfile  = f'{data_path}/scratch2.txt'\n",
        "comment_match1 = re.compile(r\"/\\*.*?\\*/\")\n",
        "comment_match2 = re.compile(r\"--+.*$\")\n",
        "\n",
        "# strip single line comments\n",
        "with open(outfile, 'w') as out_fh:\n",
        "  with open(infile, 'r') as in_fh:\n",
        "    for line in in_fh:\n",
        "      line = comment_match1.sub(r'', line)\n",
        "      line = comment_match2.sub(r'', line)\n",
        "      out_fh.write(f\"{line}\")\n",
        "\n",
        "infile = f'{data_path}/scratch2.txt'\n",
        "outfile = f'{data_path}/scratch3.txt'\n",
        "multiline_comment_match = re.compile(r\"^\\/\\*[\\s\\S]+?\\*\\/\", re.MULTILINE)\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "# strip multi line comments\n",
        "case_statements = multiline_comment_match.sub(r'', case_statements)\n",
        "case_statements = multiline_comment_match.sub(r'', case_statements)\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    out_fh.write(case_statements)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standardise format of Teradata RegExp_Similar statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "infile = f'{data_path}/scratch3.txt'\n",
        "outfile = f'{data_path}/scratch4.txt'\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "\n",
        "case_statements = re.sub(r\"WHEN\\s+\\nRegExp_Similar\",\n",
        "                         r\"WHEN RegExp_Similar\", case_statements)\n",
        "case_statements = re.sub(r\"WHEN\\s+\\n\\(\\nRegExp_Similar\",\n",
        "                         r\"WHEN (RegExp_Similar\", case_statements)\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    out_fh.write(case_statements)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Raw Case Statements (minus comments, but with uniform formatting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "euJznMeGoUs9"
      },
      "outputs": [],
      "source": [
        "infile = f'{data_path}/scratch4.txt'\n",
        "outfile = f'{data_path}/raw_case_statements.txt'\n",
        "\n",
        "# strip blanklines\n",
        "with open(outfile, 'w') as out_fh:\n",
        "  with open(infile, 'r') as in_fh:\n",
        "    for line in in_fh:\n",
        "      if not line.isspace():\n",
        "        out_fh.write(f\"{line}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Key Counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wl93JFIWoUs9",
        "outputId": "8f4e0a18-9b68-4831-95e5-09e3a72b96b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DV:4\n",
            "BG:4\n",
            "A:4\n",
            "B:4\n",
            "E:4\n",
            "CI:4\n",
            "BS:4\n",
            "DK:4\n",
            "C:4\n",
            "DI:4\n",
            "F:4\n",
            "G:4\n",
            "H:4\n",
            "J:4\n",
            "I:4\n",
            "V:4\n",
            "D:4\n",
            "BH:4\n",
            "K:4\n",
            "L:4\n",
            "M:4\n",
            "N:4\n",
            "O:4\n",
            "P:4\n",
            "Q:4\n",
            "R:4\n",
            "U:4\n",
            "S:4\n",
            "T:4\n",
            "CK:4\n",
            "W:4\n",
            "X:4\n",
            "Y:4\n",
            "Y1:4\n",
            "Z:4\n",
            "BA:4\n",
            "BB:4\n",
            "BC:4\n",
            "BF:4\n",
            "BI:4\n",
            "BJ:4\n",
            "BK:4\n",
            "BL:4\n",
            "BM:4\n",
            "BN:4\n",
            "BO:4\n",
            "BQ:4\n",
            "BR:4\n",
            "AM:4\n",
            "BT:4\n",
            "BU:4\n",
            "BV:4\n",
            "BW:4\n",
            "BX:4\n",
            "BY:4\n",
            "BZ:4\n",
            "CA:4\n",
            "CB:4\n",
            "CC:4\n",
            "CD:4\n",
            "CE:4\n",
            "CF:4\n",
            "CG:4\n",
            "CH:4\n",
            "CJ:4\n",
            "CM:4\n",
            "CN:4\n",
            "CO:4\n",
            "CP:4\n",
            "CQ:4\n",
            "CR:4\n",
            "CS:4\n",
            "CT:4\n",
            "CU:4\n",
            "CV:4\n",
            "CW:4\n",
            "CX:4\n",
            "CY:4\n",
            "CZ:4\n",
            "DA:4\n",
            "DB:4\n",
            "DC:4\n",
            "DD:4\n",
            "DE:4\n",
            "DF:4\n",
            "DG:4\n",
            "DH:4\n",
            "DJ:4\n",
            "DT:4\n",
            "CL:4\n",
            "BD:4\n",
            "BP:4\n",
            "DM:4\n",
            "BH1:4\n",
            "ZZZ:4\n",
            "DO:4\n",
            "DN:4\n",
            "DP:4\n",
            "DQ:4\n",
            "DR:4\n",
            "DS:4\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "infile = f'{data_path}/raw_case_statements.txt'\n",
        "outfile = f'{data_path}/wre_code_counts.txt'\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements = in_fh.read()\n",
        "\n",
        "# code_pattern = r\"THEN\\s+'([A-Z1]{1,3}):\\s+[\\w\\s/$&-]+'\"\n",
        "code_pattern = r\"(?:THEN\\s+')([A-Z1]{1,3}):(?:\\s+[\\w\\s/$&-]+')\"\n",
        "\n",
        "ptrn = re.compile(code_pattern, re.MULTILINE)\n",
        "codes = ptrn.findall(case_statements)\n",
        "\n",
        "# print(codes)\n",
        "\n",
        "code_map = Counter(codes)\n",
        "\n",
        "# get the key counts\n",
        "with open(outfile, 'w') as out_fh:\n",
        "    for key, value in code_map.items():\n",
        "        print(f\"{key}:{value}\")\n",
        "        out_fh.write(f\"{key}:{value}\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse Case Statements into clauses and store in a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "infile = f'{data_path}/raw_case_statements.txt'\n",
        "# extract the case statements\n",
        "case_statement_pattern = r\"(?P<case_statement>^WHEN(?P<logic>[\\s\\S\\n]+?)THEN\\s+'(?P<wre_code>[A-Z1]{1,3}):\\s+(?P<code_desc>[\\w\\s/$&-]+)')\"\n",
        "case_ptrn = re.compile(case_statement_pattern, re.MULTILINE)\n",
        "\n",
        "# extract the case statement wre code\n",
        "code_pattern = r\"^THEN\\s+'(?P<wre_code>[A-Z1]{1,3}):\\s+[\\w\\s/$&-]+'\"\n",
        "code_ptrn = re.compile(code_pattern, re.MULTILINE)\n",
        "\n",
        "# extract the wre code's description\n",
        "code_descr_rgx = r\"THEN\\s+'[A-Z1]{1,3}:\\s+(?P<code_desc>[\\w\\s/$&-]+)'\"\n",
        "code_descr_ptrn = re.compile(code_descr_rgx)\n",
        "\n",
        "# extract the Teradata RegExpSimilar clause\n",
        "regexp_rgx = r\"\\('\\s*\\|\\|\\s*'(?P<regexp>[\\w|\\n]+)'\\s*\\|\\|\\s*'\\)\"\n",
        "regexp_ptrn = re.compile(regexp_rgx, re.MULTILINE)\n",
        "\n",
        "# extract the case statement's LIKE ANY clause\n",
        "like_any_rgx = r\"LIKE ANY\\s*(?P<like_any>\\([\\w\\W\\n]+?\\))\\s*\\n(AND|OR|THEN)\"\n",
        "like_any_ptrn = re.compile(like_any_rgx, re.MULTILINE)\n",
        "\n",
        "# extract the case statement's NOT LIKE ALL clause\n",
        "not_like_all_rgx = r\"NOT LIKE ALL\\s*(?P<not_like_all>\\([\\w\\W\\n]+?\\))[\\n]*?\\s*?(THEN|OR)\"\n",
        "not_like_all_ptrn = re.compile(not_like_all_rgx, re.MULTILINE)\n",
        "\n",
        "with open(infile, 'r') as in_fh:\n",
        "    case_statements_string = in_fh.read()\n",
        "\n",
        "NUM_CASES = 101  # includes Y, Y1, BH and BH1\n",
        "#########################################################################\n",
        "# define IHC_dict dictionary to catalogue IHC case statements\n",
        "#########################################################################\n",
        "case_statement_list = [m.groupdict()\n",
        "                       for m in case_ptrn.finditer(case_statements_string)]\n",
        "IHC_dict = {statement['wre_code']: statement['case_statement']\n",
        "            for statement in case_statement_list[:NUM_CASES]}\n",
        "\n",
        "\n",
        "def strip_char(text):\n",
        "    text = re.sub(r\"['\\n\\(\\)%]\", ' ', text)\n",
        "    # text = re.sub(r\"['\\n\\(\\)]\", ' ', text)\n",
        "    # text = re.sub(r\"(\\w+)(%+)(\\w+)\", r'\\1 \\3', text)\n",
        "    # text = re.sub(r\"(\\w+)(%+)(\\w+)\", r'\\1 \\3', text)\n",
        "    return text.lstrip().rstrip()\n",
        "\n",
        "\n",
        "def assemble_clause(search_term):\n",
        "    search_term = [strip_char(text) for text in re.split(',', search_term)]\n",
        "    search_term = \"|\".join(search_term)\n",
        "    # print(f\"{search_term=}\")\n",
        "    return search_term\n",
        "\n",
        "\n",
        "# split each case statement into separate clauses\n",
        "for code, case_statement in IHC_dict.items():\n",
        "    # print(f\"{code=}\")\n",
        "    code_descr = code_descr_ptrn.search(case_statement)\n",
        "    code_descr = code_descr.group('code_desc') if bool(code_descr) else None\n",
        "\n",
        "    like_any = like_any_ptrn.search(case_statement)\n",
        "    like_any = assemble_clause(like_any.group(\n",
        "        'like_any')) if bool(like_any) else None\n",
        "    # print(f\"{like_any=}\")\n",
        "\n",
        "    regexp = regexp_ptrn.search(case_statement)\n",
        "    regexp = regexp.group('regexp') if bool(regexp) else None\n",
        "    # print(f\"{regexp=}\")\n",
        "\n",
        "    not_like_all = not_like_all_ptrn.search(case_statement)\n",
        "    not_like_all = assemble_clause(not_like_all.group(\n",
        "        'not_like_all')) if bool(not_like_all) else None\n",
        "    # print(f\"{not_like_all=}\")\n",
        "\n",
        "    #########################################################################\n",
        "    # redefine IHC_dict dictionary fields\n",
        "    #########################################################################\n",
        "    IHC_dict[code] = {\"code_descr\": code_descr,\n",
        "                      \"like_any\": like_any,\n",
        "                      \"regexp\": regexp,\n",
        "                      \"not_like_all\": not_like_all}\n",
        "\n",
        "\n",
        "# helper function to return case statemnt clauses\n",
        "def get_case_statement(wre_code, component=None):\n",
        "    # if component is not None:\n",
        "    if bool(component):\n",
        "        return IHC_dict[wre_code][component]\n",
        "    else:\n",
        "        return IHC_dict[wre_code]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'code_descr': 'Parking/Tolls', 'like_any': 'ALBERT PARK PARKING|ERSKIN PARK PARKING|EXL CAR PARKS PARKING|MACQUARIE PARK PARKING|DUDLEY PARK PARKING|NATURE PARK PARKING|PARKES PARKING|PARK MANAGEMENT PARKING|PARKVILLE PARKING|ROSNY PARK PARKING|STUART PARK PARKING|SHIFT PARK  PARKING|ARIAH PARK PARKING|PRESIDENTS PARK PARKING|ERSKINE PARK PARKING      PARKING ALBERT PARK|PARKING ERSKIN PARK|PARKING EXL CAR PARKS|PARKING MACQUARIE PARK|PARKING DUDLEY PARK|PARKING NATURE PARK|PARKING PARKES|PARKING PARK MANAGEMENT|PARKING PARKVILLE|PARKING ROSNY PARK|PARKING STUART PARK|PARKING SHIFT PARK TO|PARKING ARIAH PARK|PARKING PRESIDENTS PARK|PARKING ERSKINE PARK', 'regexp': None, 'not_like_all': None}\n"
          ]
        }
      ],
      "source": [
        "print(IHC_dict['Y1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DaXGx53IoUs_",
        "outputId": "940491cd-92f3-4eb3-ecff-a0f4bbac0891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Home Office\n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('H', 'code_descr'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BPLyOV8NoUs_",
        "outputId": "be971fac-0979-4a28-8737-52f87bd44174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "home|home office|homeoffice|Home Study|home as office|home based office|work from home|working from home|WORKING AT HOME|Work at home|WFH|WORKFROMHOME|STUDY FURNITURE|STUDY ROOM|HLP|HLP|HO|HOE|HOE|H.O.E|H/O|H/O|H O 1000*0.45|H.O.E|H|HO|H/L/P|L & P|LP|H.O.E.|H O E|H O|HO: 10 HRS/WK|HO5X48X0.52|H.O EX|HM OFF|H.O|H.O  2HRS X 48WKS X $0.52|H.O  8HRS X 48WKS X $0.52|H.O.X.|HOM OFF|H.O.X|HO: 15 H/W|HO: 10 H/W|HO: 20 HRS/WK|HOU 5 X 48 X .52|HO: 5 HRS/WK|HOU 4 X 48 X .52|HOU 3 X 48 X .52|H O 100*0.45|H OFFIS|HO: 15 HRS/WK|H.O  10HRS X 48WKS X $0.52|HOU 10 X 48 X .52|HO: 6HRS P/WK*48WKS*.52|H.O. 2HRS X 48WKS X 0.52|H.O 2*5*48W*.52|H.O.|HO: 5 H/W|H.O  3HRS X 48WKS X $0.52|H.O. 15 HRS X 48 WKS @ 52C|H O EXP|H.O 10H*48W*.52|H.O  10HRS X 40WKS X $0.52|HOU 8 X 48 X .52|H.O  5HRS X 48WKS X $0.52|HO: 5HRS*48*45C|H O 1000*0.55|HMOFF|HO10X48X0.52|H.O 1*5*48W*.52|H.O  5HRS X 40WKS X $0.52|HO: 4HRS P/WK*48WKS*.52|H.O  4HRS X 48WKS X $0.52|HO: 3HRS P/WK*48WKS*.52|HO: 2 H/W|H.O. 20 HRS X 48 WKS @ 52C|H O 1000*045|H OFF|H O|H.O|HM OFF|HO-|HO|HO:|HO HR|HO PW|HO WKS|HO WEEK|HOFF|HOME OFFICE|WFH ALLOWANCE|HME OFF|H/OFFICE|HIME OFFICE|HOFFICE|H-OFFICE|HO|WFH EXPENSES|X52CENTS|APP: WFH|FOOTREST|WFH RUNNING EXPENSES|WFH|WFH-|WFHO|WFHEXP|WFH|JOB SEEKING - WFH - COVID|COVID19 EXTRA HRS|COVID-19|COVID19|COVID|FIXED RATE|FIXED RATE METHOD|FIXED RATE WFH|HOME OFFICE - NOT COVID RELATE|HOME OFFICE CALC NON COVID|SHORTCUT|USE OF HOME OFFIC  PRE COVID|WFH NON-COVID 40HRS|WFH PRIOR TO COVID 19|WFH PRIOR TO COVID 19|WORKING FORM HOME BEFORE COVID|HL&P|COVID|SHORTCUT|SHORT CUT|80 C|FIXED RATE|0.80|MAR JUN|52C|FIXED HOURLY RATE|X 80 C|APR JUN|L&P|* 80 C|WFH|15 WKS|16 WKS|17 WKS|18 WKS\n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('H', 'like_any'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PHIjFruioUs_",
        "outputId": "c51cfb97-cf41-4be2-d02a-ff7bd00a3574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(get_case_statement('H', 'regexp'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-HfUsStMoUtA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "outfile = f'{data_path}/IHC_dict.txt'\n",
        "\n",
        "# write text description of case statement dictionary to file\n",
        "with open(outfile, 'w') as case_statement_fh:\n",
        "    case_statement_fh.write(json.dumps(IHC_dict))\n",
        "\n",
        "# read text description of case statement dictionary from file\n",
        "with open(outfile, 'r') as f:\n",
        "    dictionary_as_text = f.read()\n",
        "\n",
        "# reconstruct the case statement dictionary from its text description\n",
        "IHC_dict = json.loads(dictionary_as_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "622930c0a846cc3025e2933c7b0895291b5050626809fa6d7200ae14613946b7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
